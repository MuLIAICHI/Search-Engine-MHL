{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Let's hack together a simple text search engine\n",
        "1. Works based on document contents\n",
        "2. Learns from feedback"
      ],
      "metadata": {
        "id": "X6Lfk4uDQ1uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Document Corpus"
      ],
      "metadata": {
        "id": "tqWz7b5twbiM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InAdQYR3vC7Y"
      },
      "outputs": [],
      "source": [
        "# Initialize a list of documents to serve as our corpus for the text search engine.\n",
        "docs = [\n",
        "    '''About us. We deliver Artificial Intelligence & Machine Learning\n",
        "       solutions to solve business challenges.''',\n",
        "    '''Contact information. Email [martin davtyan at filament dot ai]\n",
        "       if you have any questions''',\n",
        "    '''Filament Chat. A framework for building and maintaining a scalable\n",
        "       chatbot capability''',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation and Comments:\n",
        "The variable docs is initialized as a list of strings, where each string represents a document in the corpus.\n",
        "* The first document talks about the services offered, focusing on Artificial Intelligence and Machine Learning solutions.\n",
        "* The second document provides contact information.\n",
        "* The third document describes a chatbot framework called \"Filament Chat.\"\n",
        "\n",
        "This list of documents will serve as the data source for our text search engine. The engine will search through these documents to find relevant information based on user queries."
      ],
      "metadata": {
        "id": "uuG8HV2_wgnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing and Tokenization"
      ],
      "metadata": {
        "id": "8ji13EspxAuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "# Create a translation table to remove punctuation\n",
        "REMOVE_PUNCTUATION_TABLE = str.maketrans({x: None for x in string.punctuation})\n",
        "\n",
        "# Initialize the TreebankWordTokenizer\n",
        "TOKENIZER = TreebankWordTokenizer()\n",
        "\n",
        "# Take an example document from the corpus\n",
        "example_doc = docs[1]\n",
        "\n",
        "# Tokenize the example document after removing punctuation\n",
        "example_doc_tokenized = TOKENIZER.tokenize(\n",
        "        example_doc.translate(REMOVE_PUNCTUATION_TABLE)\n",
        "        )\n",
        "\n",
        "# Display the tokenized document\n",
        "example_doc_tokenized\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt7MyQQ6vbna",
        "outputId": "eab5f66f-4971-45e8-b7bd-e45daf8089f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Contact',\n",
              " 'information',\n",
              " 'Email',\n",
              " 'martin',\n",
              " 'davtyan',\n",
              " 'at',\n",
              " 'filament',\n",
              " 'dot',\n",
              " 'ai',\n",
              " 'if',\n",
              " 'you',\n",
              " 'have',\n",
              " 'any',\n",
              " 'questions']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation and Comments:\n",
        "* We start by importing the required libraries: string for string manipulation, and nltk for natural language processing.\n",
        "* The TreebankWordTokenizer from the nltk library is used for tokenizing the text.\n",
        "* REMOVE_PUNCTUATION_TABLE is a translation table that will be used to remove all punctuation from the text. This is done to simplify the text for easier searching.\n",
        "* We take an example document (docs[1]) from our corpus to demonstrate the tokenization process.\n",
        "* The translate() function is used to remove punctuation from the example document, and then it is tokenized using TOKENIZER.tokenize().\n",
        "* The tokenized document is stored in example_doc_tokenized, which will be a list of words that have been separated from the original text.\n",
        "\n",
        "This step is crucial for text preprocessing, as it converts the raw text into a format that is easier to work with for text search algorithms."
      ],
      "metadata": {
        "id": "-eSCCFgnxN7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming Tokenized Words"
      ],
      "metadata": {
        "id": "YHf4ia49ychy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the PorterStemmer from nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Initialize the PorterStemmer\n",
        "STEMMER = PorterStemmer()\n",
        "\n",
        "# Stem the tokens from the example document\n",
        "example_doc_tokenized_and_stemmed = [STEMMER.stem(token) for token\n",
        "                                     in example_doc_tokenized]\n",
        "\n",
        "# Display the stemmed tokens\n",
        "example_doc_tokenized_and_stemmed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpJOkJYIwSwJ",
        "outputId": "2d090634-33e0-4f17-a1ca-980dec843d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['contact',\n",
              " 'inform',\n",
              " 'email',\n",
              " 'martin',\n",
              " 'davtyan',\n",
              " 'at',\n",
              " 'filament',\n",
              " 'dot',\n",
              " 'ai',\n",
              " 'if',\n",
              " 'you',\n",
              " 'have',\n",
              " 'ani',\n",
              " 'question']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a utility function to tokenize and stem a given string\n",
        "def tokenize_and_stem(s):\n",
        "    return [STEMMER.stem(t) for t\n",
        "            in TOKENIZER.tokenize(s.translate(REMOVE_PUNCTUATION_TABLE))]\n",
        "\n",
        "\n",
        "tokenize_and_stem(example_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcp49Ve_we9T",
        "outputId": "d60bfc2e-a53e-4564-a0aa-18dd619ac3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['contact',\n",
              " 'inform',\n",
              " 'email',\n",
              " 'martin',\n",
              " 'davtyan',\n",
              " 'at',\n",
              " 'filament',\n",
              " 'dot',\n",
              " 'ai',\n",
              " 'if',\n",
              " 'you',\n",
              " 'have',\n",
              " 'ani',\n",
              " 'question']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation and Comments:\n",
        "* We import PorterStemmer from the nltk library to perform stemming on the tokens.\n",
        "* The PorterStemmer is initialized and stored in the variable STEMMER.\n",
        "* We then stem each token from the previously tokenized example document (example_doc_tokenized) using a list comprehension. The stemmed tokens are stored in example_doc_tokenized_and_stemmed.\n",
        "* A utility function tokenize_and_stem is defined to perform both tokenization and stemming on a given string s. This function will be useful for preprocessing new documents or queries in a consistent manner.\n",
        "\n",
        "Stemming is an important step in text preprocessing as it reduces words to their root form. This makes it easier for the search engine to match queries with relevant documents, even if the words in the documents and queries are in different forms (e.g., \"running\" vs. \"run\")."
      ],
      "metadata": {
        "id": "29kk5M6xzAin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "J65WdxjwyBZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the TfidfVectorizer from scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TfidfVectorizer with custom tokenizer and English stop words\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english')\n",
        "\n",
        "# Fit the vectorizer to the corpus\n",
        "vectorizer.fit(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "0TYC3aRCyHGf",
        "outputId": "d4d234b4-7661-49ac-9327-889e28f48b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(stop_words='english',\n",
              "                tokenizer=<function tokenize_and_stem at 0x7c206455d1b0>)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;,\n",
              "                tokenizer=&lt;function tokenize_and_stem at 0x7c206455d1b0&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;,\n",
              "                tokenizer=&lt;function tokenize_and_stem at 0x7c206455d1b0&gt;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation and Comments:\n",
        "* We import TfidfVectorizer from the scikit-learn library, which will be used to convert the text documents into numerical vectors based on the Term Frequency-Inverse Document Frequency (TF-IDF) algorithm.\n",
        "* The TfidfVectorizer is initialized with the custom tokenize_and_stem function we defined earlier as the tokenizer. We also specify that English stop words should be removed from the text.\n",
        "* We then fit the vectorizer to our corpus (docs) using the fit() method. This trains the vectorizer on our corpus, allowing it to learn the vocabulary and the IDF (Inverse Document Frequency) for each term.\n",
        "\n",
        "The TF-IDF algorithm is used to weight the importance of each term in each document, relative to the entire corpus. This will be crucial for ranking documents based on their relevance to a given query."
      ],
      "metadata": {
        "id": "XBoHWmLc0Z4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the vocabulary learned by the vectorizer\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgUG7rvWyTCN",
        "outputId": "0722a4b9-340d-4b08-8101-577ad6959218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'deliv': 11,\n",
              " 'artifici': 2,\n",
              " 'intellig': 17,\n",
              " 'machin': 19,\n",
              " 'learn': 18,\n",
              " 'solut': 24,\n",
              " 'solv': 25,\n",
              " 'busi': 4,\n",
              " 'challeng': 6,\n",
              " 'contact': 9,\n",
              " 'inform': 16,\n",
              " 'email': 13,\n",
              " 'martin': 21,\n",
              " 'davtyan': 10,\n",
              " 'filament': 14,\n",
              " 'dot': 12,\n",
              " 'ai': 0,\n",
              " 'ani': 1,\n",
              " 'question': 22,\n",
              " 'chat': 7,\n",
              " 'framework': 15,\n",
              " 'build': 3,\n",
              " 'maintain': 20,\n",
              " 'scalabl': 23,\n",
              " 'chatbot': 8,\n",
              " 'capabl': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation and Comments:\n",
        "* The vocabulary_ attribute of the TfidfVectorizer object (vectorizer) contains the vocabulary that the vectorizer has learned from the corpus.\n",
        "* Each term in the vocabulary is mapped to a unique integer index. These indices will be used when creating the TF-IDF vectors for each document.\n",
        "* The vocabulary is built based on the tokenization and stemming steps we performed earlier, and it also excludes the English stop words as specified during the initialization of the TfidfVectorizer.\n",
        "\n",
        "Inspecting the vocabulary can give you an idea of the terms that the search engine will consider when ranking documents. It's a good way to verify that the text preprocessing steps have been effective in filtering out irrelevant terms and reducing words to their root forms."
      ],
      "metadata": {
        "id": "iIptMzhN0S43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Vectorization"
      ],
      "metadata": {
        "id": "4stBm0T61AFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a query string\n",
        "query = 'assistance with chatbots'\n",
        "\n",
        "# Transform the query string into a TF-IDF vector using the trained vectorizer\n",
        "query_vector = vectorizer.transform([query]).todense()\n",
        "\n",
        "# Display the dense representation of the query vector\n",
        "query_vector\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRRPsspuybB9",
        "outputId": "fe54596b-272d-41be-9923-d1cce573e941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation and Comments:\n",
        "* We define a query string query that we want to search for in our corpus. The query is \"assistance with chatbots.\"\n",
        "* We use the transform() method of the trained TfidfVectorizer (vectorizer) to convert the query into a TF-IDF vector. This vector will be in the same feature space as the vectors for our corpus documents.\n",
        "* The todense() method is used to convert the sparse matrix representation of the query vector into a dense array for easier inspection.\n",
        "\n",
        "The query vector represents the importance of each term in the query, relative to the entire corpus. This vector will be used to calculate the similarity between the query and each document in the corpus, allowing us to rank the documents based on their relevance to the query."
      ],
      "metadata": {
        "id": "5B1ETojcy2iN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Document Similarity with Query"
      ],
      "metadata": {
        "id": "B3lSmnmE1oSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Transform the corpus documents into TF-IDF vectors\n",
        "doc_vectors = vectorizer.transform(docs)\n",
        "\n",
        "# Convert the query vector to a numpy array\n",
        "query_vector_np = np.array(query_vector)\n",
        "\n",
        "# Calculate the cosine similarity between the query vector and document vectors\n",
        "similarity = cosine_similarity(query_vector_np, doc_vectors)\n",
        "# let's see what we got\n",
        "similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUmt_bJyy35r",
        "outputId": "bc4e2008-08f3-4b6d-872e-813133fd6e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.36325471]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation and Comments:\n",
        "* We import the necessary libraries: numpy for numerical operations and cosine_similarity from sklearn.metrics.pairwise for calculating the similarity between vectors.\n",
        "* We transform all the documents in our corpus (docs) into TF-IDF vectors using the transform() method of the trained TfidfVectorizer. These vectors are stored in doc_vectors.\n",
        "* The query vector (query_vector) is converted to a numpy array (query_vector_np) for compatibility with the cosine_similarity function.\n",
        "* We then calculate the cosine similarity between the query vector and each of the document vectors. The result is stored in the variable similarity.\n",
        "\n",
        "The cosine similarity measures the angle between two vectors, providing a similarity score between 0 and 1. A score closer to 1 indicates higher similarity. This will be used to rank the documents in the corpus based on their relevance to the query."
      ],
      "metadata": {
        "id": "q1Fk6DZY20AZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ranking Documents and Retrieving the Most Relevant One"
      ],
      "metadata": {
        "id": "TBU2AneA3Ohm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the indices of the documents based on their similarity scores\n",
        "ranks = (-similarity).argsort(axis=None)\n",
        "\n",
        "# Display the sorted ranks\n",
        "ranks\n",
        "\n",
        "# Output: np.array([1, 2, 0])\n",
        "\n",
        "# Retrieve the most relevant document based on the highest similarity score\n",
        "most_relevant_doc = docs[ranks[0]]\n",
        "most_relevant_doc\n"
      ],
      "metadata": {
        "id": "pc7GWfXKzucy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "33b39b97-a12e-4f69-a5aa-20528f02e138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Filament Chat. A framework for building and maintaining a scalable\\n       chatbot capability'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation and Comments:\n",
        "* We use the argsort() function from NumPy to sort the indices of the documents based on their similarity scores. The - sign is used to sort the scores in descending order, so the most similar document will be first.\n",
        "* The sorted ranks are stored in the variable ranks. In this example, the output np.array([1, 2, 0]) indicates that the document at index 1 is the most relevant, followed by the documents at indices 2 and 0.\n",
        "* We then retrieve the most relevant document using docs[ranks[0]], which fetches the document at the index corresponding to the highest similarity score.\n",
        "\n",
        "By ranking the documents based on their cosine similarity with the query, we can easily identify which document is the most relevant to the user's query. This is a crucial step in any text search engine."
      ],
      "metadata": {
        "id": "unczD3Ca3Z7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Incorporating User Feedback for Improved Ranking"
      ],
      "metadata": {
        "id": "ybgerWfy1HDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store user feedback\n",
        "feedback = {\n",
        "        'who makes chatbots': [(2, 0.), (0, 1.), (1, 1.), (0, 1.)],\n",
        "        'about page': [(0, 1.)]\n",
        "}\n",
        "\n",
        "# Calculate the cosine similarity for a new query 'who makes chatbots'\n",
        "similarity = cosine_similarity(vectorizer.transform(['who makes chatbots']), doc_vectors)\n",
        "\n",
        "# Sort the indices of the documents based on their similarity scores\n",
        "ranks = (-similarity).argsort(axis=None)\n",
        "\n",
        "# Retrieve the most relevant document based on the highest similarity score\n",
        "most_relevant_doc = docs[ranks[0]]\n",
        "most_relevant_doc\n"
      ],
      "metadata": {
        "id": "1mkMWNyy1JGC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "87b56e04-bb28-4ed2-a910-3fd8436e36c3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Filament Chat. A framework for building and maintaining a scalable\\n       chatbot capability'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation and Comments:\n",
        "* We initialize a dictionary called feedback to store user feedback. The keys are queries, and the values are lists of tuples. Each tuple contains the index of a document and a relevance score (0 for irrelevant, 1 for relevant).\n",
        "* For the new query \"who makes chatbots,\" we calculate the cosine similarity with the document vectors in the corpus using the cosine_similarity function.\n",
        "* We sort the document indices based on their similarity scores, just like before, and store them in the variable ranks.\n",
        "* The most relevant document is then retrieved using docs[ranks[0]].\n",
        "\n",
        "The feedback dictionary can be used to improve the ranking algorithm by incorporating user feedback. For example, you could adjust the similarity scores based on the feedback before sorting the ranks. This makes the search engine more adaptive and better at providing relevant results over time."
      ],
      "metadata": {
        "id": "cB0i8ih85pha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Feedback to Improve Query Matching"
      ],
      "metadata": {
        "id": "JjGJO2wVPV_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import numpy for numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# Define a new query string\n",
        "query = 'who is making chatbots information'\n",
        "\n",
        "# Extract the list of past queries from the feedback dictionary\n",
        "feedback_queries = list(feedback.keys())\n",
        "\n",
        "# Calculate the cosine similarity between the new query and past queries\n",
        "similarity = cosine_similarity(vectorizer.transform([query]),\n",
        "                               vectorizer.transform(feedback_queries))\n",
        "\n",
        "# Display the similarity scores\n",
        "similarity\n",
        "\n",
        "# Output: np.array([[0.70710678, 0.]])\n",
        "\n",
        "# Find the index of the most similar past query\n",
        "max_idx = np.argmax(similarity)\n",
        "\n",
        "# Retrieve the most similar past query based on the highest similarity score\n",
        "most_similar_past_query = feedback_queries[max_idx]\n",
        "most_similar_past_query\n"
      ],
      "metadata": {
        "id": "AzxY-T4CPX6T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a24f793b-0fb6-4cf3-fdc7-241864f8a6b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'who makes chatbots'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the indices of documents that received positive feedback for the most similar query\n",
        "pos_feedback_doc_idx = [idx for idx, feedback_value\n",
        "                        in feedback[most_similar_query]\n",
        "                        if feedback_value == 1.]\n",
        "\n",
        "# Display the indices of positively rated documents\n",
        "pos_feedback_doc_idx"
      ],
      "metadata": {
        "id": "NQXVEFrhPom-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fca598f-8a72-4260-c9ec-8be00df41857"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation and Comments:\n",
        "* We define a new query, query, that we want to search for in our corpus.\n",
        "* We then extract all the queries for which we have received feedback and store them in feedback_queries.\n",
        "* Using cosine_similarity, we calculate the similarity between the new query and the queries for which we have feedback.\n",
        "* We find the index (max_idx) of the most similar query in the feedback using np.argmax().\n",
        "* We then extract the most similar query (most_similar_query) from the feedback.\n",
        "* Finally, we find the indices of the documents that received positive feedback for the most similar query. These are stored in pos_feedback_doc_idx.\n",
        "\n",
        "By comparing the new query to past queries for which we have feedback, we can identify documents that are likely to be relevant to the new query. This is a way to make the search engine adaptive and improve its performance over time."
      ],
      "metadata": {
        "id": "L2FycNVV7nCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Positive Feedback Proportions and Adjusting Similarity"
      ],
      "metadata": {
        "id": "togERu0N8TCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Counter class from the collections module\n",
        "from collections import Counter\n",
        "\n",
        "# Count the occurrences of each document index in the positive feedback\n",
        "counts = Counter(pos_feedback_doc_idx)\n",
        "\n",
        "# Display the counts\n",
        "counts\n",
        "\n",
        "# Output: Counter({0: 2, 1: 1})\n",
        "\n",
        "# Calculate the proportion of positive feedback for each document\n",
        "pos_feedback_proportions = {\n",
        "        doc_idx: count / sum(counts.values()) for doc_idx, count in counts.items()\n",
        "}\n",
        "\n",
        "# Display the calculated proportions\n",
        "pos_feedback_proportions\n"
      ],
      "metadata": {
        "id": "Uo5ejO_iPu_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6486fae9-a4b6-4f5f-d8bf-d36491d14d45"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.6666666666666666, 1: 0.3333333333333333}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the maximum similarity value from the similarity array\n",
        "nn_similarity = np.max(similarity)\n",
        "\n",
        "# Calculate the positive feedback feature for each document\n",
        "pos_feedback_feature = [nn_similarity * pos_feedback_proportions.get(idx, 0.)\n",
        "                        for idx, _ in enumerate(docs)]\n",
        "\n",
        "# Display the positive feedback feature\n",
        "pos_feedback_feature"
      ],
      "metadata": {
        "id": "ED9bvHEqP3xt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f7ece7-be73-48ea-d3f2-d667c401ee01"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4714045207910317, 0.23570226039551584, 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation and Comments:\n",
        "* We use the Counter class from Python's collections module to count the occurrences of each document index in the positive feedback (pos_feedback_doc_idx).\n",
        "* We then calculate the proportion of positive feedback for each document by dividing the count of each document by the total count of all documents. These proportions are stored in pos_feedback_proportions.\n",
        "* We extract the maximum similarity value (nn_similarity) from the similarity array calculated earlier.\n",
        "* Finally, we calculate a \"positive feedback feature\" for each document. This feature is the product of the maximum similarity value and the proportion of positive feedback for each document. This will serve as an additional feature to adjust the similarity scores based on user feedback.\n",
        "\n",
        "By incorporating user feedback into the similarity calculation, we can make the search engine more responsive to user preferences, thereby improving the relevance of the search results over time."
      ],
      "metadata": {
        "id": "ep-QY3lx87ZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scorer\n",
        "\n",
        "To combine the positive feedback feature with the tf-idf feature we developed before, let’s summarize our code in a Scorer class that can take a collection of documents and improve scoring based on positive feedback."
      ],
      "metadata": {
        "id": "NaYKCTHiQIFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Scorer():\n",
        "    \"\"\" Scores documents for a search query based on tf-idf\n",
        "        similarity and relevance feedback\n",
        "    \"\"\"\n",
        "    def __init__(self, docs):\n",
        "        \"\"\" Initialize a scorer with a collection of documents, fit a\n",
        "            vectorizer and list feature functions\n",
        "        \"\"\"\n",
        "        self.docs = docs\n",
        "        self.vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem,\n",
        "                                          stop_words='english')\n",
        "        self.doc_tfidf = self.vectorizer.fit_transform(docs)\n",
        "        self.features = [\n",
        "            self._feature_tfidf,\n",
        "            self._feature_positive_feedback,\n",
        "        ]\n",
        "        self.feature_weights = [\n",
        "            1.,\n",
        "            2.,\n",
        "        ]\n",
        "        self.feedback = {}\n",
        "\n",
        "    def score(self, query):\n",
        "        \"\"\" Generic scoring function: for a query output a numpy array\n",
        "            of scores aligned with a document list we initialized the\n",
        "            scorer with\n",
        "        \"\"\"\n",
        "        feature_vectors = [feature(query) for feature in self.features]\n",
        "        feature_vectors_weighted = [feature * weight for feature, weight\n",
        "                                    in zip(feature_vectors, self.feature_weights)]\n",
        "        return np.sum(feature_vectors_weighted, axis=0)\n",
        "\n",
        "    def learn_feedback(self, feedback_dict):\n",
        "        \"\"\" Learn feedback in a form of `query` -> (doc index, feedback value).\n",
        "            In real life it would be an incremental procedure updating the\n",
        "            feedback object.\n",
        "        \"\"\"\n",
        "        self.feedback = feedback_dict\n",
        "\n",
        "    def _feature_tfidf(self, query):\n",
        "        \"\"\" TF-IDF feature. Return a numpy array of cosine similarities\n",
        "            between TF-IDF vectors of documents and the query\n",
        "        \"\"\"\n",
        "        query_vector = self.vectorizer.transform([query])\n",
        "        similarity = cosine_similarity(query_vector, self.doc_tfidf)\n",
        "        return similarity.ravel()\n",
        "\n",
        "    def _feature_positive_feedback(self, query):\n",
        "        \"\"\" Positive feedback feature. Search the feedback dict for a query\n",
        "            similar to the given one, then assign documents positive values\n",
        "            if there is positive feedback about them.\n",
        "        \"\"\"\n",
        "        if not self.feedback:\n",
        "            return np.zeros(len(self.docs))\n",
        "\n",
        "        feedback_queries = list(self.feedback.keys())\n",
        "        similarity = cosine_similarity(self.vectorizer.transform([query]),\n",
        "                                       self.vectorizer.transform(feedback_queries))\n",
        "        nn_similarity = np.max(similarity)\n",
        "        nn_idx = np.argmax(similarity)\n",
        "        pos_feedback_doc_idx = [idx for idx, feedback_value in\n",
        "                                self.feedback[feedback_queries[nn_idx]]\n",
        "                                if feedback_value == 1.]\n",
        "\n",
        "        feature_values = {\n",
        "                doc_idx: nn_similarity * count / sum(Counter(pos_feedback_doc_idx).values())\n",
        "                for doc_idx, count in Counter(pos_feedback_doc_idx).items()\n",
        "        }\n",
        "        return np.array([feature_values.get(doc_idx, 0.)\n",
        "                         for doc_idx, _ in enumerate(self.docs)])\n"
      ],
      "metadata": {
        "id": "72s710CTQLeO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation and Comments:\n",
        "* The Scorer class is designed to rank documents based on their relevance to a given query.\n",
        "* The __init__ method initializes the class with a collection of documents, fits a TF-IDF vectorizer, and sets up feature functions and their weights.\n",
        "* The score method calculates the relevance score for each document based on the features and their weights.\n",
        "* The learn_feedback method allows the class to learn from user feedback.\n",
        "* The _feature_tfidf method calculates the TF-IDF feature for each document.\n",
        "* The _feature_positive_feedback method calculates the positive feedback feature for each document based on user feedback.\n",
        "\n",
        "This class encapsulates the logic for scoring and ranking documents, making it easier to manage and extend the functionality of the search engine. It also allows for the incorporation of user feedback to improve the relevance of search results over time."
      ],
      "metadata": {
        "id": "K2rf4BBL-yUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Scorer class with the document corpus\n",
        "scorer = Scorer(docs)\n",
        "\n",
        "# Define a query\n",
        "query = 'who is making chatbots information'\n",
        "\n",
        "# Score the documents for the query\n",
        "initial_scores = scorer.score(query)\n",
        "initial_scores"
      ],
      "metadata": {
        "id": "6wrOIcQyQPzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd88339-4f2b-41a5-ffe7-857b57ec50db"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.22847492, 0.25685987])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the most relevant document based on the initial scores\n",
        "initial_most_relevant_doc = docs[initial_scores.argmax()]\n",
        "print(\"Initial most relevant document:\", initial_most_relevant_doc)"
      ],
      "metadata": {
        "id": "7-Ar65a0QbnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2792f923-d9cd-4586-ba8d-38e1dbd2f09a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial most relevant document: Filament Chat. A framework for building and maintaining a scalable\n",
            "       chatbot capability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide feedback to the scorer\n",
        "scorer.learn_feedback(feedback)\n",
        "\n",
        "# Re-score the documents for the query after learning the feedback\n",
        "updated_scores = scorer.score(query)\n",
        "\n",
        "# Display the updated scores\n",
        "print(\"Updated scores:\", updated_scores)\n",
        "\n",
        "# Retrieve the most relevant document based on the updated scores\n",
        "updated_most_relevant_doc = docs[updated_scores.argmax()]\n",
        "print(\"Updated most relevant document:\", updated_most_relevant_doc)"
      ],
      "metadata": {
        "id": "c7XM77AQQcbr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23ad0ae-e032-4780-8ebd-50f4dab3df47"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated scores: [0.94280904 0.69987944 0.25685987]\n",
            "Updated most relevant document: About us. We deliver Artificial Intelligence & Machine Learning\n",
            "       solutions to solve business challenges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation and Comments:\n",
        "* We initialize an instance of the Scorer class with our document corpus (docs).\n",
        "* We define a query (query) that we want to search for in our corpus.\n",
        "* We use the score method of the Scorer class to calculate the relevance scores for each document in the corpus. These scores are stored in initial_scores.\n",
        "We then retrieve the most relevant document based on these initial scores using argmax().\n",
        "* Next, we provide feedback to the scorer using the learn_feedback method. This updates the internal feedback dictionary of the Scorer class.\n",
        "* We re-score the documents using the score method again, but this time the scores are influenced by the feedback. These updated scores are stored in updated_scores.\n",
        "* Finally, we retrieve the most relevant document based on these updated scores.\n",
        "\n",
        "By using the Scorer class, we can easily rank documents based on their relevance to a query and update these rankings based on user feedback. This makes the search engine more adaptive and better at providing relevant results over time."
      ],
      "metadata": {
        "id": "U1pB_lHj_5CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Adjusting Feature Weights and Re-scoring Documents"
      ],
      "metadata": {
        "id": "0mX2wK0dAokT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the feature weights in the Scorer class\n",
        "scorer.feature_weights = [0.6, 0.4]\n",
        "\n",
        "# Re-score the documents for the query with the updated feature weights\n",
        "adjusted_scores = scorer.score(query)\n",
        "\n",
        "# Retrieve the most relevant document based on the adjusted scores\n",
        "adjusted_most_relevant_doc = docs[adjusted_scores.argmax()]\n",
        "print(\"Most relevant document with adjusted weights:\", adjusted_most_relevant_doc)\n"
      ],
      "metadata": {
        "id": "0UACsjPNQtdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64631c03-ee9a-4abd-e0a4-5462856e6643"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most relevant document with adjusted weights: Contact information. Email [martin davtyan at filament dot ai]\n",
            "       if you have any questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explanation and Comments:\n",
        "* We update the feature_weights attribute of the Scorer class to [0.6, 0.4]. This means that the TF-IDF feature will now have a weight of 0.6, and the positive feedback feature will have a weight of 0.4.\n",
        "* We then re-score the documents using the score method of the Scorer class. The scores are now influenced by the updated feature weights and are stored in adjusted_scores.\n",
        "* Finally, we retrieve the most relevant document based on these adjusted scores using argmax().\n",
        "\n",
        "By adjusting the feature weights, we can fine-tune the importance of different features in the scoring algorithm. This allows us to experiment with different combinations of feature weights to find the optimal balance for ranking documents based on their relevance to a query."
      ],
      "metadata": {
        "id": "oUuzjkanAt6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "In this notebook, we've walked through the process of building a simple yet effective text search engine. Starting with a basic corpus of documents, we employed various Natural Language Processing techniques such as tokenization, stemming, and TF-IDF vectorization to transform the text data into a format suitable for machine learning.\n",
        "\n",
        "We then introduced the concept of cosine similarity to measure the relevance of each document to a given query. This provided us with a baseline search engine capable of ranking documents based on their content.\n",
        "\n",
        "Recognizing the importance of adaptability in search engines, we incorporated a feedback mechanism. This allowed the system to learn from user interactions, thereby improving the quality of search results over time. We encapsulated all these functionalities into a Scorer class, making it easier to manage and extend.\n",
        "\n",
        "Finally, we explored the impact of feature weighting, demonstrating how different aspects of the scoring algorithm can be fine-tuned for optimal performance.\n",
        "\n",
        "The end result is a search engine that not only ranks documents based on their relevance to a query but also adapts to user feedback to provide increasingly accurate results. While simple, this framework serves as a solid foundation upon which more complex and robust search engines can be built.\n",
        "\n",
        "By understanding and implementing these core concepts, you're well on your way to diving deeper into the fascinating world of search engines and information retrieval."
      ],
      "metadata": {
        "id": "-Qy48uJVBFZE"
      }
    }
  ]
}